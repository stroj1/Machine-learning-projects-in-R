---
title: "DABN_variable_selection"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```




## Task 1)
In this task we are going to explore which variables are relevant for first run U.S. box office ($) sales, for a set of 62 movies.
We have 12 explanatory variables:

* MPRating = MPAA Rating code, 1=G, 2=PG, 3=PG13, 4=R,
* Budget = Production budget ($Mil),
* Starpowr = Index of star poser,
* Sequel = 1 if movie is a sequel, 0 if not,
* Action = 1 if action film, 0 if not,
* Comedy = 1 if comedy film, 0 if not,
* Animated = 1 if animated film, 0 if not,
* Horror = 1 if horror film, 0 if not,
* Addict = Trailer views at traileraddict.com,
* Cmngsoon = Message board comments at comingsoon.net,
* Fandango = Attention at fandango.com (see Example 4.12),
* Cntwait3 = Percentage of Fandango votes that can't wait to see


```{r }
data <- read.csv('movie_buzz.txt')
```


## Task 1 a)


Create a `lm` object using all the 12 explanatory variables vs *logarithm* (`log` not `log10` in R) of Box office sales (colnames `BOX` in the data). Also use the `summary` function to extract the p-values read the help file for `summary.lm` to figure out where p-values are stored.
```{r }
lm.obj <- lm(log(BOX)~.,data=data)
sum.obj  <- summary.lm(lm.obj)
p.values <- sum.obj$coefficients[,4]


```


## Task 1 b)

Perform variable selection using Holm's procedure through the `p.adjust()` function for for a family-wise error rate of $\alpha=0.05$. Store the names of the selected variables.
```{r }
alpha=0.05
p.holm <-  p.adjust(p.values,method ="holm")
names.selected <- names(which(p.holm<=alpha))
```

## Task 2)

In this task you will create a function that from a linear regression model builds your own $p-$values. You will in Task 3 perform simulations to get a feeling how variable selection works in a linear regression setting. To do this you will need efficient numerical algorithm since you will do many simulations. Building efficient code is a very important part of ML as many methods are very time consuming.


We begin by extracting the relevant matrices as usual:
```{r }
X <- model.matrix(log(BOX)~ 1 + ., data = data)
y <- log(data$BOX)
```


## Task 2a)

In this task you are supposed to calculate all statistics manually by implementing their mathematical expressions, i.e. don't use `lm`.
First calculate OLS coefficients $\hat{\beta}$ and their marginal variance. Note that  
$$ \widehat{V[\hat{\beta} | X, y]}= \hat{\sigma}^{2}(X^TX)^{-1} $$ where $\hat{\sigma}^2$ is the estimated variance of the residuals.
(A good way to see that you done things correctly is to compare your result with `summary(lm.obj)`)

```{r }
 beta.hat <- solve(t(X)%*%X)%*%(t(X)%*%y)
 sigma2.hat <- sum((y-X%*%beta.hat)^2)/(nrow(X)-ncol(X))
 VX <- diag(sigma2.hat*solve(t(X)%*%X)) #marginal variance not the full covariance matrix
 

 
 
```

## Task 2b)

Now we are going to create the P-values. Create $t-$ statistic for the $\beta_i$ under $H_0:\beta_i=0$ namely
$$
t_i = \left|\frac{\hat{\beta}_i}{\widehat{sd[\hat{\beta}_i | X, y]}} \right|
$$

and also compute the corresponding $p-$value here use that $\frac{\hat{\beta}_i}{\widehat{sd[\hat{\beta}_i | X, y]}} $ follows a student-t distribution with $n-p$ degrees of freedom (hint you need to use `pt`, and you can control that you get the correct answer by comparing with `summary(lm.obj)` ). Hint use that
$$
P_{0}(|T| \geq t) = P_{0}(T \leq -t)  + P_{0}(T \geq t).
$$

```{r }
 t <- abs(beta.hat/sqrt(VX))
 p <- pt(q=-t,df=nrow(X)-ncol(X))+pt(q=t,df=nrow(X)-ncol(X),lower.tail = FALSE)
```


## Task 2c)
Now in task 3 you will simulate new $Y$ but keep the matrix $X$ fixed, and you will extract $p$-values from a regression of simulated $Y$ on existing $X$. To do so efficiently we want to precompute as many objects as possible. Create a function that as efficiently as possible returns the $p-$values for a new y.  All operations that involve $X$ alone must not be conducted inside the function. Instead, their results should be fed into the function as additional inputs.

```{r }
m.beta<-solve(t(X)%*%X)%*%t(X)
m.VX<-solve(t(X)%*%X)
df<-nrow(X)-ncol(X)


 calculate.p <- function(y, X,m.beta,m.VX,df){ # add the extra variables 

   beta.hat <-m.beta%*%y
   sigma2.hat <- sum((y-X%*%beta.hat)^2)/df
   VX <- diag(sigma2.hat*m.VX)
   
   t <- abs(beta.hat/sqrt(VX))
   p.values <-pt(-t, df)+pt(t, df, lower.tail=F)
   
   
 return(p.values)
}

```


## Task 2d)
Write a loop that in each iteration simulates a `y.fake` assuming a normal distribution with mean and variance coming from the observed $y$.
Further compute the $p-$value of this new observation using $X$ and your function `caclulate.p`. Do a variable selection using both Bonferroni and Holm and store the number of selected variables except for intercept at level $\alpha=0.05$. Use this to compute the FWER. Does either of Holm's or Bonferroni's methods successfully control the FWER?

```{r }
n.sim = 100
alpha = 0.05
n.selected.Holm <- rep(0, n.sim)
n.selected.BF   <- rep(0, n.sim) 
start.time <- Sys.time()


for( i in 1:n.sim){
   y.fake <- rnorm(length(y), mean=mean(y),sd=sd(y))
   p.vals <- calculate.p(y.fake, X,m.beta,m.VX,df) 
    p.holm <- p.adjust(p.vals,"holm")[-1]
    p.BF    <- p.adjust(p.vals,"bonferroni")[-1]
  n.selected.Holm[i] <- sum(p.holm <=alpha) 
  n.selected.BF[i]   <- sum(p.BF <=alpha)
}
 
 FWER.Holm<-sum(n.selected.Holm>0)/n.sim
 FWER.BF<- sum(n.selected.BF>0)/n.sim
 
 end.time <- Sys.time()
 cat(n.sim, 'iterations took' , (end.time-start.time), 'sec')

 
 task2f.dotheycontrolFWER <- "Both methods control FWER"
```



## Task 2e)
Now compute the same loop using `lm` inside the loop to get the p-values rather then your own function.
What explains the difference in speed?
```{r }

 data.fake <- data
 for( i in 1:n.sim){
    y.fake <- rnorm(length(y), mean=mean(y),sd=sd(y))
    data.fake$BOX <- exp(y.fake)
    lm.obj.fake <- lm(y.fake~.,data = data)
    p.holm <- p.adjust(p.vals,"holm")
    p.BF    <- p.adjust(p.vals,"bonferroni")
    n.selected.Holm[i] <- sum(p.holm <=alpha)
    n.selected.BF[i]   <- sum(p.BF <=alpha)
}
 end.time <- Sys.time()
 cat(n.sim, 'iterations took' , (end.time-start.time), 'sec')
 
 task2e.differenceinspeed <- "1. because the lm formula calculate so much more things, for example the residuals. 2. because in calculate.p we have X fixed, because we calculated some things outside of the funciton"
```

## Task 3)

Now that we have created an efficient algorithm for computing the p-values we are going to explore what happens when one adds several signals to the simulated data.

But for comparison among singles simpler we first standardize the variables (but not center)
```{r }
library(caret)
pre.obj <- preProcess(X, method="scale")
X.scaled <- predict(pre.obj, X)
```


## Task 3a)
Now that the variances of all predictors have been set to one, we need to obtain the inputs to our function `calculate.p` for this scaled data. Do this in the code chunk below

```{r }

m.beta.scaled<-solve(t(X.scaled)%*%X.scaled)%*%(t(X.scaled))
m.VX.scaled<-solve(t(X.scaled)%*%X.scaled)
df.scaled<-nrow(X.scaled)-ncol(X.scaled)

```



## Task 3b)
Next, we simulate fake outcomes. However, in contrast to task 2d we are not creating new $Y$ that are completely unrelated to the observed predictors in our box office sales data. More specifically, we simulate $Y$ from the linear model
$$
\mathbf{y}= \mathbf{X}\boldsymbol{\beta} + \boldsymbol{\varepsilon}
$$
where $X$ contains the scaled predictors of Task 3a). For the vector of slope coefficients $\beta$, we let $\beta_{2:5}=log(2:5)$ whereas all other elements of this vector are 0. The model errors $\epsilon$ are drawn independently from a standard normal distribution. Construct a new object `y.fake` that contains simulated outcomes from the model described here.
```{r }
set.seed(4456)
signal.index <- c(2,3,4,5)
beta.fake <- rep(0, dim(X)[2])

beta.fake[signal.index] <- log(signal.index)
y.fake <- X.scaled%*%beta.fake+rnorm(nrow(X.scaled),mean = 0,sd=1)

```

Now use your `calculate.p` function and `p.adjust` to the significant variables using Bonferroni and
Benjamini & Hochberg correction at level $\alpha=0.15$ also calculate the False discovery ratio $fdr$

```{r }
set.seed(4456)
alpha = 0.15
p.vals       <- calculate.p(y.fake, X.scaled,m.beta.scaled,m.VX.scaled,df.scaled)
p.BF         <- p.adjust(p.vals,"bonferroni")[-1]
p.hochberg   <- p.adjust(p.vals,"BH")[-1]
selected.BF  <- p.BF <=alpha
selected.hochberg <- p.hochberg <=alpha
selected.true.BF <- c(selected.BF[signal.index])
selected.true.hochberg <- c(selected.hochberg[signal.index])
fdp.BF   <-  sum(selected.BF[-signal.index])/sum(selected.BF)
fdp.hochberg   <-  sum(selected.hochberg[-signal.index])/sum(selected.hochberg)

```



## Task 3c)

We are now going to create a double loop where we for each magnitude of the signals compute the 
$FWER$, $FDR$ and the power of each signal. Verify if both methods controls the $FDR$.
In order to calculate the estimated $FWER$, $FDR$, and power it is convenient to store if the signal has been selected in a zero-one matrix (one if the variable is selected) in the inner loop. Then in the outer loop the three statistics one can computed  using the two matrices `selected.BF` and `selected.hochberg`.

```{r }
set.seed(12334)
alpha = 0.15
sim = 1000
n <- dim(X)[1]
p <- dim(X)[2]
magnitudes <- seq(0,1,length.out = 20)
   
Power.Hochberg <- Power.BF         <- matrix(0, length(magnitudes),length(signal.index) )
FWER.Hochberg  <- FWER.BF          <- rep(0, length(magnitudes))
FDR.BF         <- FDR.Hochberg     <-  rep(0, length(magnitudes))
selected.BF <- selected.hochberg <- matrix(0, sim, p)
for( i in 1:length(magnitudes)){
 mag <- magnitudes[i]
 for( ii in 1:sim){
   beta.fake.i <- mag * beta.fake
   y.fake <- rnorm(n, X.scaled%*%beta.fake.i, 1)
   p.vals       <- calculate.p(y.fake, X.scaled,m.beta.scaled,m.VX.scaled,df.scaled)
   p.BF         <- p.adjust(p.vals,"bonferroni")
   p.hochberg   <- p.adjust(p.vals,"BH")
   
    selected.BF[ii,] <- p.BF<=alpha
    selected.hochberg[ii,] <- p.hochberg<=alpha
   }
  Power.BF[i,]     <- colSums(selected.BF[,signal.index])/sim
  Power.Hochberg[i,] <- colSums(selected.hochberg[,signal.index])/sim
  FWER.BF[i]       <- mean(rowSums(selected.BF[,-c(1,signal.index)])>0)
  FWER.Hochberg[i]   <- mean(rowSums(selected.hochberg[,-c(1,signal.index)])>0)
  FDR.BF[i]        <- mean(rowSums(selected.BF[,-c(1,signal.index)])/(max(rowSums(selected.BF[,-1]),1)))
  FDR.Hochberg[i]   <-  mean(rowSums(selected.hochberg[,-c(1,signal.index)])/(max(rowSums(selected.hochberg[,-1]),1)))
}



```


## Task 3d)

We plot the power of the strongest and weakest signal vs the magnitude of the signal, have both BF and Hochberg in the same plot. Why is the difference between method larger on the weakest signal?

```{r }
plot(magnitudes,Power.BF[,1], type='l', ylim=c(0,1))
lines(magnitudes,Power.Hochberg[,1], col='red')
plot(magnitudes,Power.BF[,4], type='l', ylim=c(0,1))
lines(magnitudes,Power.Hochberg[,4], col='red')
Task3d.why.difference <- "Since we have the p-values ordered, we will have the strongest signal in the 1st position and the weakest in the 4th position. In the strongest signal, the p-value will be low for both methods, the rejection area will be similar. The stronger the signal is, the lower the p-value and the more likely is that we will reject the null hypothesis. 
Hochberg is a method that is sensitive to the magnitude of the signal, whereas Bonferroni does not depend on the magnitude of the signal because we will always have the rejection boundary such that we reject when the p-value is lower than alpha/n. The implication is that as we increase the p-value, the rejection area of the Hochberg procedure will increase too. The power is the indicator of how good we are rejecting the right null hypotheses, so in the weakest signal, where the rejection area for both methods is different, we will have that power is higher for hochberg because the rejection area is bigger and is more likely that we will reject the null hypothesis"
```

## Task 3e)

Now plot the FWER rates for both BF and Hochberg. What are the pattern in both lines? What causes the patterns?

```{r }
plot(magnitudes,FWER.BF, type='l', ylim=c(0,1))
lines(magnitudes,FWER.Hochberg, col='red')

Task3d.what.pattern<- "We can see in the graph that Bonferroni remains stable across the magnitudes while Hochberg increases as the magnitud increases"
Task3d.why.pattern<- "By definition FWER is the probability of rejecting any true null hypothesis, or in other words is the risk of including noise variables. The noise variables remain unchanged with changes in p-value, whereas the signal variable's p-value is reduced as the magnitud increases. Therefore, the signal variables are pushed to the left and noise variables are pushed to the right. Since the Hochberg threshold increases with the magnitud, a noise variable that was not included before can be included now. In other words, it increases the risk of including noise variables that were not included before, and that means that the FWER increases."
```

## Task 4)
In the article Mullainathan, Sendhil, and Jann Spiess. 2017. "Machine Learning: An Applied Econometric Approach." Journal of Economic Perspectives, 31 (2): 87-106, the authors compared several ML techniques for prediction of house prices (log of house prices). Here we will explore this data sets for variable selection (slightly cleaned by us a priori).
First we load the data and extract the relevant data.
```{r }
data.ahs <- readRDS('ahs2011forjep.rdata')
formula <- as.formula(data.ahs$getformula(data.ahs$vars))
X <- model.matrix(formula, data.ahs$df)
X <- X[,-1]
y <- data.ahs$df$LOGVALUE
```



## Task 4a)
As usual when working with lasso, we rescale the variances of all predictors and demean all variables before training any model.

```{r }
 scale.X  <- preProcess(X,method = c("center", "scale"))
 X         <- predict(scale.X,X)
 y         <- y-mean(y)
 
```

## Task 4b)
Now use `glmnet` to perform ten-fold cross-validation select $\lambda$ through the one sd rule. Then extract the variables selected (i.e. the variables corresponding to the non-zero coefficients of the lasso fit with selected lambda)
```{r }
library(glmnet)
 
 cv<-cv.glmnet(X,y,alpha=1,type.measure = "mse", intercept = FALSE, nfolds = 10)
 beta.coeff <-  coef(cv, s = "lambda.1se")
 selected.names <- rownames(beta.coeff)[as.vector(beta.coeff)!=0]
 number.selected <- length(selected.names)
```


## Task 4c)
Now read the tutorial  [https://web.stanford.edu/group/candes/knockoffs/software/knockoffs/tutorial-3-r.html](https://web.stanford.edu/group/candes/knockoffs/software/knockoffs/tutorial-3-r.html)
and perform controlled variables using knockoffs with the statistics.  
$$
Z_i  =max \{\lambda:\beta_j(\lambda) \neq 0\}.
$$
Here use the fixed-X option for knockoffs, and set offset to zero.
```{r }
library(knockoff)

fdr <- 0.15

result.4c <- knockoff.filter(X, y, knockoffs = create.fixed, statistic = stat.glmnet_lambdasmax, fdr = fdr, offset = 0)

selected.names.knockoff <- names(result.4c$selected)
number.selected <- length(selected.names.knockoff)


```





